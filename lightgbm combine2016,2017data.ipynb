{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import time\n",
    "import datetime as dt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datapath_2016 = \"./zillow data/merged_2016 v4.csv\"\n",
    "train_datapath_2017 = \"./zillow data/merged_2017 v4.csv\"\n",
    "\n",
    "test_datapath_2016 = \"./zillow data/properties_2016 v4.csv\"\n",
    "test_datapath_2017 = \"./zillow data/properties_2017 v4.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify 'transactiondate' of df to keep only the month\n",
    "def parse_transactiondate(df):\n",
    "    # modify transactiondate col to keep only the month\n",
    "    transactiondate_year = []\n",
    "    transactiondate_month = [] \n",
    "    transactiondate_quarter = []\n",
    "    for data in df['transactiondate']:\n",
    "        temp = data.split(\"-\")\n",
    "        year = int(temp[0])\n",
    "        transactiondate_year.append(year) # keep only month\n",
    "        month = int(temp[1])\n",
    "        month = (year - 2016)*12 + month\n",
    "        transactiondate_month.append(month)\n",
    "        quarter = (year-2016)*4 + int((month-1)/3) + 1\n",
    "        transactiondate_quarter.append(quarter)\n",
    "        \n",
    "    transactiondate_year = np.array(transactiondate_year)\n",
    "    transactiondate_year = transactiondate_year.reshape(transactiondate_year.shape[0], 1)\n",
    "    transactiondate_month = np.array(transactiondate_month)\n",
    "    transactiondate_month = transactiondate_month.reshape(transactiondate_month.shape[0], 1)\n",
    "    transactiondate_quarter = np.array(transactiondate_quarter)\n",
    "    transactiondate_quarter = transactiondate_month.reshape(transactiondate_quarter.shape[0], 1)\n",
    "\n",
    "    df['transactiondate_year'] = transactiondate_year\n",
    "    df['transactiondate_month'] = transactiondate_month\n",
    "    df['transactiondate_quarter'] = transactiondate_quarter\n",
    "    df = df.drop('transactiondate',axis = 1)\n",
    "    return df\n",
    "\n",
    "# return label index, feature index list\n",
    "# assumes that df label_is_first\n",
    "def split_on_label(df):\n",
    "    return df.columns[0], df.columns[1:]\n",
    "\n",
    "def gen_testdata(df, transactiondate_year,transactiondate_month,transactiondate_quarter):\n",
    "    df_copy = df.copy()\n",
    "    # df_copy['transactiondate'] = new_transactiondate\n",
    "    df_copy.insert(0, 'transactiondate_year', transactiondate_year)\n",
    "    df_copy.insert(0, 'transactiondate_month', transactiondate_month)\n",
    "    df_copy.insert(0, 'transactiondate_quarter', transactiondate_quarter)\n",
    "    return df_copy\n",
    "\n",
    "def get_low_var_feature(support_list):\n",
    "    low_var_feature_index = []\n",
    "    support_feature_index = []\n",
    "    for i in range(len(support_list)):\n",
    "        if support_list[i]:\n",
    "            support_feature_index.append(i)\n",
    "        else:\n",
    "            low_var_feature_index.append(i)\n",
    "    return low_var_feature_index, support_feature_index\n",
    "\n",
    "\n",
    "def handle_low_var(sel, train_data, feature_list):\n",
    "    temp_sel = sel.fit(train_data)\n",
    "    low_var_list, support_list = get_low_var_feature(sel.get_support())\n",
    "\n",
    "    return low_var_list\n",
    "\n",
    "\n",
    "# Removing Features with more than 98% missing data\n",
    "def removing_missing(X_train, missing_threshold = 0.95):\n",
    "    exclude_missing = []\n",
    "    num_rows = X_train.shape[0]\n",
    "    for col in X_train.columns:\n",
    "        num_missing = X_train[col].isna().sum()\n",
    "        if num_missing == 0:\n",
    "            continue\n",
    "        missing_fraction = num_missing/float(num_rows)\n",
    "        if missing_fraction > missing_threshold:\n",
    "            exclude_missing.append(col)\n",
    "            \n",
    "    return exclude_missing\n",
    "\n",
    "# removing features with unique value\n",
    "def removing_unique(X_train):\n",
    "    exclude_unique = []\n",
    "    for col in X_train.columns:\n",
    "        num_unique = len(X_train[col].unique())\n",
    "        if X_train[col].isna().sum()!=0 and num_unique == 1:\n",
    "            exclude_unique.append(col)\n",
    "            \n",
    "    return exclude_unique\n",
    "\n",
    "def print_list(li,list_name = ''):\n",
    "    print('%s (%d) :' %(list_name,len(li)))\n",
    "    for i in li:\n",
    "        print(i)\n",
    "        \n",
    "def get_train_features(X_train,el1 = [], el2 = [], el3 = [],el4 = []):\n",
    "    train_feature = []\n",
    "    for col in X_train:\n",
    "        if col not in el1 and col not in el2 and col not in el3 and col not in el4:\n",
    "            train_feature.append(col)\n",
    "    return train_feature\n",
    "\n",
    "# geting categorical features\n",
    "def get_cat_feature(X_train, cat_threshold):\n",
    "    cat_feature_inds = []\n",
    "    cat_feature = []\n",
    "    for i,col in enumerate(X_train):\n",
    "        num_uniques = len(X_train[col].unique())\n",
    "        if num_uniques < cat_threshold and not 'sqft' in col and not 'cnt' in col \\\n",
    "        and not 'nbr' in col and not 'number' in col:\n",
    "            cat_feature_inds.append(i)\n",
    "            cat_feature.append(col)\n",
    "    return cat_feature_inds,cat_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016] num of features: 60\n",
      "[2016] num of instances: 167888\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "train_data16 = pd.read_csv(train_datapath_2016, index_col=0)\n",
    "train_data17 = pd.read_csv(train_datapath_2017, index_col=0)\n",
    "\n",
    "# drop parcelid (index of properties features)\n",
    "train_data16 = train_data16.drop('parcelid', axis=1)\n",
    "train_data17 = train_data17.drop('parcelid', axis=1)\n",
    "\n",
    "# modify 'transactiondate' to keep only the month\n",
    "train_data16 = parse_transactiondate(train_data16)\n",
    "train_data17 = parse_transactiondate(train_data17)\n",
    "\n",
    "train_data = pd.concat([train_data16,train_data17],axis = 0)\n",
    "\n",
    "label, feature = split_on_label(train_data)\n",
    "\n",
    "# training data and label for model 2016\n",
    "X_train = train_data[feature]\n",
    "\n",
    "y_train = train_data[label]\n",
    "\n",
    "del train_data16, train_data17\n",
    "gc.collect()\n",
    "\n",
    "print(\"[2016] num of features:\", len(feature))\n",
    "print(\"[2016] num of instances:\", train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Freatures with too many missing data\n",
    "exclude_missing = removing_missing(X_train)\n",
    "\n",
    "# Removing Features with unique values\n",
    "exclude_unique = removing_unique(X_train)\n",
    "\n",
    "# Removing Features with low variance\n",
    "# it should remove features with many missing values that were filled by imputation\n",
    "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "#exclude_low_var16 = handle_low_var(sel, X_train16, feature16)\n",
    "#exclude_low_var17 = handle_low_var(sel, X_train17, feature17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/venv/lib/python3.6/site-packages/pandas/core/frame.py:3790: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  downcast=downcast, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_train.fillna(-999,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclude_missing (18) :\n",
      "architecturalstyletypeid\n",
      "basementsqft\n",
      "buildingclasstypeid\n",
      "decktypeid\n",
      "finishedsquarefeet13\n",
      "finishedsquarefeet15\n",
      "finishedsquarefeet6\n",
      "hashottuborspa\n",
      "poolsizesum\n",
      "pooltypeid10\n",
      "pooltypeid2\n",
      "storytypeid\n",
      "typeconstructiontypeid\n",
      "yardbuildingsqft17\n",
      "yardbuildingsqft26\n",
      "fireplaceflag\n",
      "taxdelinquencyflag\n",
      "taxdelinquencyyear\n",
      "exclude_unique (0) :\n"
     ]
    }
   ],
   "source": [
    "print_list(exclude_missing,'exclude_missing')\n",
    "#print_list(exclude_low_var16,'exclude_low_var16')\n",
    "print_list(exclude_unique,'exclude_unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2016] num of features: 42\n",
      "train_feature16 (42) :\n",
      "airconditioningtypeid\n",
      "bathroomcnt\n",
      "bedroomcnt\n",
      "buildingqualitytypeid\n",
      "calculatedbathnbr\n",
      "finishedfloor1squarefeet\n",
      "calculatedfinishedsquarefeet\n",
      "finishedsquarefeet12\n",
      "finishedsquarefeet50\n",
      "fips\n",
      "fireplacecnt\n",
      "fullbathcnt\n",
      "garagecarcnt\n",
      "garagetotalsqft\n",
      "heatingorsystemtypeid\n",
      "latitude\n",
      "longitude\n",
      "lotsizesquarefeet\n",
      "poolcnt\n",
      "pooltypeid7\n",
      "propertycountylandusecode\n",
      "propertylandusetypeid\n",
      "propertyzoningdesc\n",
      "rawcensustractandblock\n",
      "regionidcity\n",
      "regionidcounty\n",
      "regionidneighborhood\n",
      "regionidzip\n",
      "roomcnt\n",
      "threequarterbathnbr\n",
      "unitcnt\n",
      "yearbuilt\n",
      "numberofstories\n",
      "structuretaxvaluedollarcnt\n",
      "taxvaluedollarcnt\n",
      "assessmentyear\n",
      "landtaxvaluedollarcnt\n",
      "taxamount\n",
      "censustractandblock\n",
      "transactiondate_year\n",
      "transactiondate_month\n",
      "transactiondate_quarter\n"
     ]
    }
   ],
   "source": [
    "train_feature = get_train_features(X_train, el1 = exclude_missing, \n",
    "                                                #el2 = exclude_low_var16,\n",
    "                                                el3 = exclude_unique)\n",
    "\n",
    "new_X_train = X_train[train_feature]\n",
    "\n",
    "print(\"\\n[2016] num of features:\", new_X_train.shape[1])\n",
    "print_list(train_feature,\"train_feature16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_feature (16) :\n",
      "airconditioningtypeid\n",
      "buildingqualitytypeid\n",
      "fips\n",
      "heatingorsystemtypeid\n",
      "pooltypeid7\n",
      "propertycountylandusecode\n",
      "propertylandusetypeid\n",
      "regionidcity\n",
      "regionidcounty\n",
      "regionidneighborhood\n",
      "regionidzip\n",
      "yearbuilt\n",
      "assessmentyear\n",
      "transactiondate_year\n",
      "transactiondate_month\n",
      "transactiondate_quarter\n"
     ]
    }
   ],
   "source": [
    "cat_feature_inds, cat_feature = get_cat_feature(new_X_train, cat_threshold = 1000)\n",
    "print_list(cat_feature,'cat_feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feauture Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomSearchCV\n",
    "def handle_lightgbm_RCV(X, y, param_dict,N):\n",
    "    \n",
    "    # cross-validate on alpha (regularization strenght) from alphalist\n",
    "    \n",
    "    gbm = lgb.LGBMRegressor(n_jobs = -1, random_state = 42,silent = False)\n",
    "       \n",
    "    # cross-validate on random search CV\n",
    "    random_search = RandomSearchCV(gbm,param_distributions=param_dict,\n",
    "                                 n_iter = N,random_state = 21,\n",
    "                                 cv=5, scoring='neg_mean_absolute_error',verbose = True)\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X, y)\n",
    "    print(\"RandomSearchCV took %.2f seconds\"\n",
    "          \" parameter settings.\" % (time.time() - start))\n",
    "\n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_lightgbm_GridCV(X, y, param_dict):\n",
    "    \n",
    "    # cross-validate on alpha (regularization strenght) from alphalist\n",
    "    \n",
    "    gbm = lgb.LGBMRegressor(n_jobs = -1, random_state = 42,silent = False)\n",
    "       \n",
    "    # cross-validate on random search CV\n",
    "    grid_search = GridSearchCV(gbm,param_grid=param_dict,\n",
    "                               cv=5, scoring='neg_mean_absolute_error',verbose = True)\n",
    "    \n",
    "    start = time.time()\n",
    "    grid_search.fit(X, y)\n",
    "    print(\"GridSearchCV took %.2f seconds\"\n",
    "          \" parameter settings.\" % (time.time() - start))\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 15.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 961.70 seconds parameter settings.\n"
     ]
    }
   ],
   "source": [
    "params_dict = {\n",
    "    \"learning_rate\":[0.014],\n",
    "    'num_leaves':[140],\n",
    "    'max_depth':[-1],\n",
    "    'reg_alpha':[0.01],\n",
    "    'reg_lambda':[100],\n",
    "    'min_split_gain':[0],\n",
    "    'min_child_samples':[100],\n",
    "    'colsample_bytree':[0.8],\n",
    "    \n",
    "    'min_child_weight':[0.001],\n",
    "    'subsample':[0.8],\n",
    "    'subsample_for_bin':[50000],\n",
    "    'subsample_freq':[1], \n",
    "    'n_eatimators':[512],\n",
    "    'ramdon_state':[21]\n",
    "}\n",
    "grid_result = handle_lightgbm_GridCV(new_X_train, y_train, params_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/venv/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[mean: -0.06937, std: 0.00190, params: {'colsample_bytree': 0.8, 'learning_rate': 0.001, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06932, std: 0.00189, params: {'colsample_bytree': 0.8, 'learning_rate': 0.0014384498882876629, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06926, std: 0.00188, params: {'colsample_bytree': 0.8, 'learning_rate': 0.00206913808111479, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06918, std: 0.00186, params: {'colsample_bytree': 0.8, 'learning_rate': 0.002976351441631319, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06909, std: 0.00185, params: {'colsample_bytree': 0.8, 'learning_rate': 0.004281332398719396, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06900, std: 0.00184, params: {'colsample_bytree': 0.8, 'learning_rate': 0.006158482110660267, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06893, std: 0.00183, params: {'colsample_bytree': 0.8, 'learning_rate': 0.008858667904100823, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06890, std: 0.00181, params: {'colsample_bytree': 0.8, 'learning_rate': 0.012742749857031334, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06892, std: 0.00180, params: {'colsample_bytree': 0.8, 'learning_rate': 0.018329807108324356, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06902, std: 0.00180, params: {'colsample_bytree': 0.8, 'learning_rate': 0.026366508987303583, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06923, std: 0.00183, params: {'colsample_bytree': 0.8, 'learning_rate': 0.0379269019073225, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.06959, std: 0.00184, params: {'colsample_bytree': 0.8, 'learning_rate': 0.05455594781168517, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.07011, std: 0.00182, params: {'colsample_bytree': 0.8, 'learning_rate': 0.07847599703514611, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.07090, std: 0.00185, params: {'colsample_bytree': 0.8, 'learning_rate': 0.11288378916846883, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.07205, std: 0.00180, params: {'colsample_bytree': 0.8, 'learning_rate': 0.1623776739188721, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.07391, std: 0.00181, params: {'colsample_bytree': 0.8, 'learning_rate': 0.23357214690901212, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.07673, std: 0.00167, params: {'colsample_bytree': 0.8, 'learning_rate': 0.3359818286283781, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.08104, std: 0.00212, params: {'colsample_bytree': 0.8, 'learning_rate': 0.4832930238571752, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.08790, std: 0.00195, params: {'colsample_bytree': 0.8, 'learning_rate': 0.6951927961775606, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1},\n",
       " mean: -0.09981, std: 0.00244, params: {'colsample_bytree': 0.8, 'learning_rate': 1.0, 'max_depth': -1, 'min_child_samples': 100, 'min_child_weight': 0.001, 'min_split_gain': 0, 'n_eatimators': 512, 'num_leaves': 140, 'ramdon_state': 21, 'reg_alpha': 0.01, 'reg_lambda': 100, 'subsample': 0.8, 'subsample_for_bin': 50000, 'subsample_freq': 1}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/venv/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/apple/venv/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, ylim=None, cv=None, train_sizes=np.linspace(.05, 1., 20), verbose=0, plot=True):\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "            estimator, X, y, cv=cv, train_sizes=train_sizes, scoring = 'neg_mean_absolute_error',verbose=verbose)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.xlabel(u\"number of samples\")\n",
    "        plt.ylabel(u\"score\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid()\n",
    "        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                         alpha=0.1, color=\"b\")\n",
    "        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                         alpha=0.1, color=\"r\")\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=u\"training set score\")\n",
    "        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=u\"validation set score\")\n",
    "\n",
    "        plt.legend(loc=\"best\")\n",
    "\n",
    "        plt.draw()\n",
    "        plt.show()\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "    midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2\n",
    "    diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])\n",
    "    return midpoint, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-8f4c53cda7ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                            )\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_learning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-469a51b5c56a>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[0;34m(estimator, X, y, ylim, cv, train_sizes, verbose, plot)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(\n\u001b[0;32m----> 4\u001b[0;31m             estimator, X, y, cv=cv, train_sizes=train_sizes, scoring = 'neg_mean_absolute_error',verbose=verbose)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/learning_curve.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[0;34m(estimator, X, y, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, error_score)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 171\u001b[0;31m             for train, test in cv for n_train_samples in train_sizes_abs)\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    632\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m                                        callbacks=callbacks)\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    472\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 474\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    202\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1527\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bst = lgb.LGBMRegressor(n_jobs = -1, random_state = 42,silent = False,\n",
    "                            learning_rate = 0.014,\n",
    "                            n_estimators = 512,\n",
    "                            num_leaves = 140,\n",
    "                            reg_alpha = 0.01,\n",
    "                            reg_lambda = 100,\n",
    "                            min_split_gain = 0,\n",
    "                            min_child_samples = 100,\n",
    "                            subsample = 0.8,\n",
    "                            subsample_for_bin = 50000,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            subsample_freq = 1,\n",
    "                            max_depth = -1,\n",
    "                            min_child_weight = 0.001\n",
    "                        \n",
    "                           )\n",
    "start = time.time()\n",
    "_,_ = plot_learning_curve(bst, new_X_train, y_train)\n",
    "during_time = time.time() - start\n",
    "print(during_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/venv/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016] num of instances:  2985217\n",
      "[2017] num of instances:  2985217\n"
     ]
    }
   ],
   "source": [
    "# read in test\n",
    "test_data2016 = pd.read_csv(test_datapath_2016, index_col=0)\n",
    "test_data2017 = pd.read_csv(test_datapath_2017, index_col=0)\n",
    "\n",
    "print(\"[2016] num of instances: \", test_data2016.shape[0])\n",
    "print(\"[2017] num of instances: \", test_data2017.shape[0])\n",
    "\n",
    "# save parcelid for merge \n",
    "test_parcelid16 = test_data2016['parcelid']\n",
    "test_parcelid17 = test_data2017['parcelid']\n",
    "\n",
    "# drop parcelid col\n",
    "test_data2016 = test_data2016.drop('parcelid', axis=1)\n",
    "test_data2017 = test_data2017.drop('parcelid', axis=1)\n",
    "\n",
    "# generate transaction date\n",
    "test_year = np.repeat(2016, test_data2016.shape[0])\n",
    "test_quarter = np.repeat(4, test_data2016.shape[0])\n",
    "\n",
    "test_month10_16 = np.repeat(10, test_data2016.shape[0])\n",
    "test_month11_16 = np.repeat(11, test_data2016.shape[0])\n",
    "test_month12_16 = np.repeat(12, test_data2016.shape[0])\n",
    "\n",
    "test_month10_17 = np.repeat(22, test_data2017.shape[0])\n",
    "test_month11_17 = np.repeat(23, test_data2017.shape[0])\n",
    "test_month12_17 = np.repeat(24, test_data2017.shape[0])\n",
    "\n",
    "# get new test data with transaction date\n",
    "X_test10_16 = gen_testdata(test_data2016, test_year, test_month10_16,test_quarter)\n",
    "X_test11_16 = gen_testdata(test_data2016, test_year, test_month11_16,test_quarter)\n",
    "X_test12_16 = gen_testdata(test_data2016, test_year, test_month12_16,test_quarter)\n",
    "\n",
    "X_test10_17 = gen_testdata(test_data2017, test_year, test_month10_17,test_quarter)\n",
    "X_test11_17 = gen_testdata(test_data2017, test_year, test_month11_17,test_quarter)\n",
    "X_test12_17 = gen_testdata(test_data2017, test_year, test_month12_17,test_quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test10_16 = X_test10_16[train_feature]\n",
    "X_test11_16 = X_test11_16[train_feature]\n",
    "X_test12_16 = X_test12_16[train_feature]\n",
    "X_test10_17 = X_test10_17[train_feature]\n",
    "X_test11_17 = X_test11_17[train_feature]\n",
    "X_test12_17 = X_test12_17[train_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(X_train,y_train,X_test):\n",
    "    start = time.time()\n",
    "    bst = lgb.LGBMRegressor(n_jobs = -1, random_state = 42,silent = False,\n",
    "                            learning_rate = 0.014,\n",
    "                            n_estimators = 512,\n",
    "                            num_leaves = 140,\n",
    "                            reg_alpha = 0.01,\n",
    "                            reg_lambda = 100,\n",
    "                            min_split_gain = 0,\n",
    "                            min_child_samples = 100,\n",
    "                            subsample = 0.8,\n",
    "                            subsample_for_bin = 50000,\n",
    "                            colsample_bytree = 0.8,\n",
    "                            subsample_freq = 1,\n",
    "                            max_depth  = -1,\n",
    "                            min_child_weight = 0.001\n",
    "                           )\n",
    "    bst.fit(X_train, y_train)\n",
    "    y_pred = bst.predict(X_test)\n",
    "    print('time using:%.2f'%(time.time()-start))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time using:222.55\n",
      "time using:196.50\n",
      "time using:217.11\n",
      "time using:168.77\n",
      "time using:187.18\n",
      "time using:185.88\n"
     ]
    }
   ],
   "source": [
    "y_pred10_16 = train_and_test(new_X_train, y_train, X_test10_16.values)\n",
    "y_pred11_16 = train_and_test(new_X_train, y_train, X_test11_16.values)\n",
    "y_pred12_16 = train_and_test(new_X_train, y_train, X_test12_16.values)\n",
    "y_pred10_17 = train_and_test(new_X_train, y_train, X_test10_17.values)\n",
    "y_pred11_17 = train_and_test(new_X_train, y_train, X_test11_17.values)\n",
    "y_pred12_17 = train_and_test(new_X_train, y_train, X_test12_17.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Parcelid    201610    201611    201612    201710    201711    201712\n",
      "0         10754147  0.028049  0.028431  0.029355  0.049639  0.049639  0.049639\n",
      "1         10759547 -0.004185 -0.003239 -0.001811  0.013211  0.013211  0.013211\n",
      "2         10843547  0.049635  0.049635  0.049635  0.103998  0.103998  0.103998\n",
      "3         10859147  0.092716  0.091380  0.091508  0.127437  0.127437  0.127437\n",
      "4         10879947  0.027588  0.027588  0.028027  0.054558  0.054558  0.054558\n",
      "5         10898347  0.032808  0.032840  0.032611  0.048364  0.048364  0.048364\n",
      "6         10933547 -0.012797 -0.013247 -0.013965  0.025217  0.025217  0.025217\n",
      "7         10940747  0.038732  0.038614  0.040091  0.058608  0.058608  0.058608\n",
      "8         10954547  0.043967  0.044350  0.045274  0.064318  0.064318  0.064318\n",
      "9         10976347 -0.000247 -0.000357  0.001120  0.029871  0.029871  0.029871\n",
      "10        11073947  0.003286  0.003830  0.004527  0.025780  0.025780  0.025780\n",
      "11        11114347  0.018199  0.018010  0.018200  0.037514  0.037514  0.037514\n",
      "12        11116947 -0.015289 -0.013641 -0.013452  0.005903  0.005903  0.005903\n",
      "13        11142747 -0.009960 -0.009585 -0.008661  0.029054  0.029054  0.029054\n",
      "14        11193347  0.002202  0.002014  0.002938  0.030589  0.030589  0.030589\n",
      "15        11215747  0.005914  0.006289  0.007213  0.037064  0.037064  0.037064\n",
      "16        11229347  0.064072  0.064072  0.064575  0.076461  0.076461  0.076461\n",
      "17        11287347  0.015328  0.015832  0.018130  0.048105  0.048105  0.048105\n",
      "18        11288547  0.050706  0.049995  0.050178  0.089920  0.089920  0.089920\n",
      "19        11324547  0.005925  0.005851  0.006118  0.032029  0.032029  0.032029\n",
      "20        11391347  0.084935  0.084584  0.083839  0.081892  0.081892  0.081892\n",
      "21        11395747  0.065134  0.064104  0.063260  0.065419  0.065419  0.065419\n",
      "22        11404347 -0.004418 -0.005607 -0.005728  0.007097  0.007097  0.007097\n",
      "23        11405747  0.006858  0.007254  0.008226  0.038043  0.038043  0.038043\n",
      "24        11417147 -0.022726 -0.021941 -0.018429  0.032012  0.032012  0.032012\n",
      "25        11457547 -0.049805 -0.047267 -0.045968 -0.024125 -0.024125 -0.024125\n",
      "26        11488147 -0.004852 -0.004739 -0.005445  0.018051  0.018051  0.018051\n",
      "27        11520747  0.055412  0.055870  0.055100  0.101228  0.101228  0.101228\n",
      "28        11524947  0.009432  0.009781  0.009929  0.010371  0.010371  0.010371\n",
      "29        11544747  0.014527  0.014041  0.013701  0.026668  0.026668  0.026668\n",
      "...            ...       ...       ...       ...       ...       ...       ...\n",
      "2985187  167636430 -0.107497 -0.106404 -0.106675  0.007425  0.007425  0.007425\n",
      "2985188  167690630 -0.107497 -0.106404 -0.106675  0.017522  0.017522  0.017522\n",
      "2985189  167636630 -0.107497 -0.106404 -0.106675  0.018751  0.018751  0.018751\n",
      "2985190   10834030 -0.107497 -0.106404 -0.106675  0.038850  0.038850  0.038850\n",
      "2985191  167637430 -0.107497 -0.106404 -0.106675  0.044426  0.044426  0.044426\n",
      "2985192  167637630 -0.107497 -0.106404 -0.106675  0.235611  0.235611  0.235611\n",
      "2985193  167637230 -0.107497 -0.106404 -0.106675  0.032963  0.032963  0.032963\n",
      "2985194   11645030 -0.107497 -0.106404 -0.106675  0.015367  0.015367  0.015367\n",
      "2985195  167689030 -0.107497 -0.106404 -0.106675  0.035714  0.035714  0.035714\n",
      "2985196  167638630 -0.107497 -0.106404 -0.106675  0.020956  0.020956  0.020956\n",
      "2985197  167638430 -0.107497 -0.106404 -0.106675  0.030188  0.030188  0.030188\n",
      "2985198   14342030 -0.107497 -0.106404 -0.106675  0.157966  0.157966  0.157966\n",
      "2985199  167638230 -0.107497 -0.106404 -0.106675  0.069180  0.069180  0.069180\n",
      "2985200  167637830 -0.107497 -0.106404 -0.106675  0.040485  0.040485  0.040485\n",
      "2985201  167639230 -0.107497 -0.106404 -0.106675  0.033996  0.033996  0.033996\n",
      "2985202   14341030 -0.107497 -0.106404 -0.106675  0.104267  0.104267  0.104267\n",
      "2985203   14341630 -0.107497 -0.106404 -0.106675  0.123310  0.123310  0.123310\n",
      "2985204   14367630 -0.107497 -0.106404 -0.106675  0.047818  0.047818  0.047818\n",
      "2985205  167638830 -0.107497 -0.106404 -0.106675  0.012361  0.012361  0.012361\n",
      "2985206   12572230 -0.107497 -0.106404 -0.106675  0.028810  0.028810  0.028810\n",
      "2985207   14460030 -0.107497 -0.106404 -0.106675  0.018409  0.018409  0.018409\n",
      "2985208   14284830 -0.107497 -0.106404 -0.106675  0.082278  0.082278  0.082278\n",
      "2985209   14285230 -0.107497 -0.106404 -0.106675  0.104172  0.104172  0.104172\n",
      "2985210   14455630 -0.107497 -0.106404 -0.106675  0.120609  0.120609  0.120609\n",
      "2985211   11117630 -0.107497 -0.106404 -0.106675  0.025976  0.025976  0.025976\n",
      "2985212  168176230 -0.107497 -0.106404 -0.106675  0.130852  0.130852  0.130852\n",
      "2985213   14273630 -0.107497 -0.106404 -0.106675  0.046544  0.046544  0.046544\n",
      "2985214  168040630 -0.107497 -0.106404 -0.106675  0.041310  0.041310  0.041310\n",
      "2985215  168040830 -0.107497 -0.106404 -0.106675  0.088904  0.088904  0.088904\n",
      "2985216  168040430 -0.107497 -0.106404 -0.106675  0.050423  0.050423  0.050423\n",
      "\n",
      "[2985217 rows x 7 columns]\n",
      "2985217 7\n"
     ]
    }
   ],
   "source": [
    "# merged on Parcelid for predicted result on test data\n",
    "\n",
    "test_dict_16 = {'Parcelid': test_parcelid16, '201610': y_pred10_16, '201611': y_pred11_16, '201612': y_pred12_16}\n",
    "test_dict_17 = {'Parcelid': test_parcelid17, '201710': y_pred10_17, '201711': y_pred11_17, '201712': y_pred12_17}\n",
    "\n",
    "df_test_16 = pd.DataFrame(data=test_dict_16)\n",
    "df_test_17 = pd.DataFrame(data=test_dict_17)\n",
    "\n",
    "df_merged = df_test_16.merge(df_test_17, left_on='Parcelid', right_on='Parcelid', how='outer')\n",
    "print(df_merged)\n",
    "\n",
    "# handle submission file\n",
    "submitfile = \"./zillow data/sample_submission.csv\"\n",
    "\n",
    "submit_df = pd.read_csv(submitfile)\n",
    "print(submit_df.shape[0], submit_df.shape[1])\n",
    "\n",
    "assert(submit_df.shape[0] == df_merged.shape[0]), \"Error: invalid row size for submit!\"\n",
    "df_merged.to_csv(\"./zillow data/new_submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv(\"./zillow data/new_draft_submission1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
