{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datapath_2016 = \"./zillow-data/merged_2016 v2.csv\"\n",
    "train_datapath_2017 = \"./zillow-data/merged_2017 v2.csv\"\n",
    "\n",
    "test_datapath_2016 = \"./zillow-data/properties_2016 v2.csv\"\n",
    "test_datapath_2017 = \"./zillow-data/properties_2017 v2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify 'transactiondate' of df to keep only the month\n",
    "def parse_transactiondate(df):\n",
    "    # modify transactiondate col to keep only the month\n",
    "    new_transactiondate = []\n",
    "    for data in df['transactiondate']:\n",
    "        temp = data.split(\"-\")\n",
    "        new_transactiondate.append(int(temp[1])) # keep only month\n",
    "\n",
    "    new_transactiondate = np.array(new_transactiondate)\n",
    "    new_transactiondate = new_transactiondate.reshape(new_transactiondate.shape[0], 1)\n",
    "\n",
    "    df['transactiondate'] = new_transactiondate\n",
    "\n",
    "# return label index, feature index list\n",
    "# assumes that df label_is_first\n",
    "def split_on_label(df):\n",
    "    return df.columns[0], df.columns[1:]\n",
    "\n",
    "def gen_testdata(df, new_transactiondate):\n",
    "    df_copy = df.copy()\n",
    "    # df_copy['transactiondate'] = new_transactiondate\n",
    "    df_copy.insert(0, 'transactiondate', new_transactiondate)\n",
    "    return df_copy\n",
    "\n",
    "def get_low_var_feature(support_list):\n",
    "    low_var_feature_index = []\n",
    "    support_feature_index = []\n",
    "    for i in range(len(support_list)):\n",
    "        if support_list[i]:\n",
    "            support_feature_index.append(i)\n",
    "        else:\n",
    "            low_var_feature_index.append(i)\n",
    "    return low_var_feature_index, support_feature_index\n",
    "\n",
    "def handle_low_var(sel, train_data, feature_list):\n",
    "    temp_sel = sel.fit(train_data)\n",
    "    low_var_list, support_list = get_low_var_feature(sel.get_support())\n",
    "    \n",
    "    for i in low_var_list:\n",
    "        print(feature_list[i])\n",
    "    return temp_sel, support_list\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016] num of features: 58\n",
      "[2016] num of instances: 90275\n",
      "[2017] num of features: 58\n",
      "[2017] num of instances: 77613\n"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "train_data16 = pd.read_csv(train_datapath_2016, index_col=0)\n",
    "train_data17 = pd.read_csv(train_datapath_2017, index_col=0)\n",
    "\n",
    "# drop parcelid (index of properties features)\n",
    "train_data16 = train_data16.drop('parcelid', axis=1)\n",
    "train_data17 = train_data17.drop('parcelid', axis=1)\n",
    "\n",
    "label16, feature16 = split_on_label(train_data16)\n",
    "label17, feature17 = split_on_label(train_data17)\n",
    "\n",
    "print(\"[2016] num of features:\", len(feature16))\n",
    "print(\"[2016] num of instances:\", train_data16.shape[0])\n",
    "\n",
    "print(\"[2017] num of features:\", len(feature17))\n",
    "print(\"[2017] num of instances:\", train_data17.shape[0])\n",
    "\n",
    "# modify 'transactiondate' to keep only the month\n",
    "parse_transactiondate(train_data16)\n",
    "parse_transactiondate(train_data17)\n",
    "\n",
    "# training data and label for model 2016\n",
    "X_train16 = train_data16[feature16]\n",
    "y_train16 = train_data16[label16]\n",
    "\n",
    "# training data and label for model 2017\n",
    "X_train17 = train_data17[feature17]\n",
    "y_train17 = train_data17[label17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(np.array(X_train16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(np.array(X_train17)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Var Features 2016:\n",
      "architecturalstyletypeid\n",
      "buildingclasstypeid\n",
      "decktypeid\n",
      "fireplacecnt\n",
      "hashottuborspa\n",
      "poolcnt\n",
      "pooltypeid10\n",
      "pooltypeid2\n",
      "pooltypeid7\n",
      "storytypeid\n",
      "threequarterbathnbr\n",
      "typeconstructiontypeid\n",
      "numberofstories\n",
      "fireplaceflag\n",
      "assessmentyear\n",
      "taxdelinquencyflag\n",
      "\n",
      "Low Var Features 2017:\n",
      "architecturalstyletypeid\n",
      "buildingclasstypeid\n",
      "decktypeid\n",
      "fireplacecnt\n",
      "hashottuborspa\n",
      "poolcnt\n",
      "pooltypeid10\n",
      "pooltypeid2\n",
      "pooltypeid7\n",
      "storytypeid\n",
      "threequarterbathnbr\n",
      "typeconstructiontypeid\n",
      "numberofstories\n",
      "fireplaceflag\n",
      "assessmentyear\n",
      "taxdelinquencyflag\n",
      "\n",
      "[2016] num of features: 42\n",
      "[2017] num of features: 42\n"
     ]
    }
   ],
   "source": [
    "# Removing Features with low variance\n",
    "# it should remove features with many missing values that were filled by imputation\n",
    "\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "\n",
    "print(\"Low Var Features 2016:\")\n",
    "sel16, support_list16 = handle_low_var(sel, X_train16, feature16)\n",
    "\n",
    "print(\"\\nLow Var Features 2017:\")\n",
    "sel17, support_list17 = handle_low_var(sel, X_train17, feature17)\n",
    "    \n",
    "new_X_train16 = sel16.transform(X_train16)\n",
    "new_X_train17 = sel17.transform(X_train17)\n",
    "\n",
    "assert(len(support_list16) == new_X_train16.shape[1]), \"Error: Invalid Size\"\n",
    "assert(len(support_list17) == new_X_train17.shape[1]), \"Error: Invalid Size\"\n",
    "\n",
    "new_feature16 = feature16[support_list16]\n",
    "new_feature17 = feature17[support_list17]\n",
    "\n",
    "print(\"\\n[2016] num of features:\", new_X_train16.shape[1])\n",
    "print(\"[2017] num of features:\", new_X_train17.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_XGBoost_RFECV(X_train, y_train):\n",
    "    # alpha values from 0.01 to 1000 up to 5 values log scale\n",
    "    alphalist = np.logspace(-2, 4, 5, endpoint=True)\n",
    "    \n",
    "    error_dict = {}\n",
    "    feature_dict = {}\n",
    "\n",
    "    # cross-validate on alpha (regularization strenght) from alphalist\n",
    "    for i in alphalist:\n",
    "        print(\"alpha:\", i)\n",
    "        bst = xgb.sklearn.XGBRegressor(max_depth=3, learning_rate=0.1, reg_alpha=i)\n",
    "        \n",
    "        # cross-validate on num of features selected using RFECV\n",
    "        # step parameter is number of features to remove at each step\n",
    "        selector = RFECV(bst, step=10, cv=5)\n",
    "        # selector = RFE(bst, 7, step=5)\n",
    "        selector = selector.fit(X_train, y_train)\n",
    "        \n",
    "        # selected feature index\n",
    "        feature = []\n",
    "        for j in range(len(selector.support_)):\n",
    "            if selector.support_[j]:\n",
    "                feature.append(j)\n",
    "        feature_dict[i] = feature\n",
    "                \n",
    "        # cross_val_score on the underlying estimator array of scores\n",
    "        score = cross_val_score(selector.estimator_, X_train[:,feature], y_train, \n",
    "                                cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "        print(\"cv score:\", -score)\n",
    "        error_dict[i] = -score\n",
    "\n",
    "    opt_a = min(error_dict, key= error_dict.get)\n",
    "    opt_feature = feature_dict[opt_a]\n",
    "\n",
    "    print(\"alpha (lambda) from CV: {}\".format(opt_a))\n",
    "    print(\"size of opt_feature from CV:\", len(opt_feature))\n",
    "\n",
    "    # obtain train MAE with CV'ed alpha value\n",
    "    bst = xgb.sklearn.XGBRegressor(reg_alpha=opt_a)\n",
    "    bst.fit(X_train[:,opt_feature], y_train)\n",
    "    y_pred = bst.predict(X_train[:,opt_feature])\n",
    "    print(\"Train MAE: {:.6}\".format(mean_absolute_error(y_train, y_pred)))\n",
    "    \n",
    "    return bst, opt_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model 2016:\n",
      "alpha: 0.01\n",
      "cv score: 0.068606078336347\n",
      "alpha: 0.31622776601683794\n",
      "cv score: 0.0684698396432951\n",
      "alpha: 10.0\n",
      "cv score: 0.06830471008195879\n",
      "alpha: 316.22776601683796\n",
      "cv score: 0.06890066387241485\n",
      "alpha: 10000.0\n",
      "cv score: 0.16259024496151384\n",
      "alpha (lambda) from CV: 10.0\n",
      "size of opt_feature from CV: 22\n",
      "Train MAE: 0.0678517\n",
      "Elasped time: 00:24:48\n"
     ]
    }
   ],
   "source": [
    "# XGBoost with RFE CV for 2016\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"XGBoost Model 2016:\")\n",
    "new_bst16, new_opt_feature = handle_XGBoost_RFECV(new_X_train16, y_train16)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"Elasped time: %H:%M:%S\", time.gmtime(elapsed_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features from RFE (2016):\n",
      "transactiondate\n",
      "bedroomcnt\n",
      "calculatedfinishedsquarefeet\n",
      "finishedsquarefeet12\n",
      "heatingorsystemtypeid\n",
      "latitude\n",
      "longitude\n",
      "lotsizesquarefeet\n",
      "propertycountylandusecode\n",
      "propertylandusetypeid\n",
      "propertyzoningdesc\n",
      "rawcensustractandblock\n",
      "regionidcity\n",
      "regionidneighborhood\n",
      "regionidzip\n",
      "yearbuilt\n",
      "structuretaxvaluedollarcnt\n",
      "taxvaluedollarcnt\n",
      "landtaxvaluedollarcnt\n",
      "taxamount\n",
      "taxdelinquencyyear\n",
      "censustractandblock\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected Features from RFE (2016):\")\n",
    "for i in new_opt_feature:\n",
    "    print(new_feature16[i])\n",
    "    \n",
    "opt_feature16_index = new_opt_feature\n",
    "opt_feature16 = new_feature16[new_opt_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model 2017:\n",
      "alpha: 0.01\n",
      "cv score: 0.07081324816015105\n",
      "alpha: 0.31622776601683794\n",
      "cv score: 0.07201245552315816\n",
      "alpha: 10.0\n",
      "cv score: 0.07043204263804788\n",
      "alpha: 316.22776601683796\n",
      "cv score: 0.0715665909445019\n",
      "alpha: 10000.0\n",
      "cv score: 0.18765495150177647\n",
      "alpha (lambda) from CV: 10.0\n",
      "size of opt_feature from CV: 22\n",
      "Train MAE: 0.0702628\n",
      "Elasped time: 00:19:48\n"
     ]
    }
   ],
   "source": [
    "# XGBoost with RFE CV for 2017\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"XGBoost Model 2017:\")\n",
    "new_bst17, new_opt_feature2 = handle_XGBoost_RFECV(new_X_train17, y_train17)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"Elasped time: %H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features from RFE (2017):\n",
      "transactiondate\n",
      "bedroomcnt\n",
      "calculatedfinishedsquarefeet\n",
      "finishedsquarefeet12\n",
      "heatingorsystemtypeid\n",
      "latitude\n",
      "longitude\n",
      "lotsizesquarefeet\n",
      "propertycountylandusecode\n",
      "propertylandusetypeid\n",
      "propertyzoningdesc\n",
      "rawcensustractandblock\n",
      "regionidcity\n",
      "regionidneighborhood\n",
      "regionidzip\n",
      "yearbuilt\n",
      "structuretaxvaluedollarcnt\n",
      "taxvaluedollarcnt\n",
      "landtaxvaluedollarcnt\n",
      "taxamount\n",
      "taxdelinquencyyear\n",
      "censustractandblock\n"
     ]
    }
   ],
   "source": [
    "print(\"Selected Features from RFE (2017):\")\n",
    "for i in new_opt_feature:\n",
    "    print(new_feature17[i])\n",
    "    \n",
    "opt_feature17_index = new_opt_feature2\n",
    "opt_feature17 = new_feature17[new_opt_feature2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016] num of instances:  2985217\n",
      "[2017] num of instances:  2985217\n"
     ]
    }
   ],
   "source": [
    "# read in test\n",
    "test_data2016 = pd.read_csv(test_datapath_2016, index_col=0)\n",
    "test_data2017 = pd.read_csv(test_datapath_2017, index_col=0)\n",
    "\n",
    "print(\"[2016] num of instances: \", test_data2016.shape[0])\n",
    "print(\"[2017] num of instances: \", test_data2017.shape[0])\n",
    "\n",
    "# save parcelid for merge \n",
    "test_parcelid16 = test_data2016['parcelid']\n",
    "test_parcelid17 = test_data2017['parcelid']\n",
    "\n",
    "# drop parcelid col\n",
    "test_data2016 = test_data2016.drop('parcelid', axis=1)\n",
    "test_data2017 = test_data2017.drop('parcelid', axis=1)\n",
    "\n",
    "# generate transaction date\n",
    "test10_16 = np.repeat(10, test_data2016.shape[0])\n",
    "test11_16 = np.repeat(11, test_data2016.shape[0])\n",
    "test12_16 = np.repeat(12, test_data2016.shape[0])\n",
    "\n",
    "test10_17 = np.repeat(10, test_data2017.shape[0])\n",
    "test11_17 = np.repeat(11, test_data2017.shape[0])\n",
    "test12_17 = np.repeat(12, test_data2017.shape[0])\n",
    "\n",
    "# get new test data with transaction date\n",
    "X_test10_16 = gen_testdata(test_data2016, test10_16)\n",
    "X_test11_16 = gen_testdata(test_data2016, test11_16)\n",
    "X_test12_16 = gen_testdata(test_data2016, test12_16)\n",
    "\n",
    "X_test10_17 = gen_testdata(test_data2017, test10_17)\n",
    "X_test11_17 = gen_testdata(test_data2017, test11_17)\n",
    "X_test12_17 = gen_testdata(test_data2017, test12_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "\n",
    "y_pred10_16 = new_bst16.predict(np.array(X_test10_16[opt_feature16]))\n",
    "y_pred11_16 = new_bst16.predict(np.array(X_test11_16[opt_feature16]))\n",
    "y_pred12_16 = new_bst16.predict(np.array(X_test12_16[opt_feature16]))\n",
    "\n",
    "y_pred10_17 = new_bst17.predict(np.array(X_test10_17[opt_feature17]))\n",
    "y_pred11_17 = new_bst17.predict(np.array(X_test11_17[opt_feature17]))\n",
    "y_pred12_17 = new_bst17.predict(np.array(X_test12_17[opt_feature17]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Parcelid    201610    201611    201612    201710    201711    201712\n",
      "0         10754147  0.025181  0.025181  0.025181  0.061004  0.061004  0.061004\n",
      "1         10759547 -0.032385 -0.032385 -0.032385  0.021161  0.021161  0.021161\n",
      "2         10843547  0.049178  0.049178  0.049178  0.018709  0.018709  0.018709\n",
      "3         10859147  0.009108  0.009108  0.009108  0.022996  0.022996  0.022996\n",
      "4         10879947 -0.001519 -0.001519 -0.001519  0.006760  0.006760  0.006760\n",
      "5         10898347  0.009677  0.009677  0.009677  0.015958  0.015958  0.015958\n",
      "6         10933547  0.001214  0.001214  0.001214  0.009431  0.009431  0.009431\n",
      "7         10940747  0.004977  0.004977  0.004977  0.010451  0.010451  0.010451\n",
      "8         10954547  0.022996  0.022996  0.022996  0.064069  0.064069  0.064069\n",
      "9         10976347  0.004818  0.004818  0.004818  0.011785  0.011785  0.011785\n",
      "10        11073947  0.000491  0.000491  0.000491  0.009674  0.009674  0.009674\n",
      "11        11114347  0.021408  0.021408  0.021408  0.056208  0.056208  0.056208\n",
      "12        11116947 -0.014304 -0.014304 -0.014304  0.013172  0.013172  0.013172\n",
      "13        11142747  0.019908  0.019908  0.019908  0.059341  0.059341  0.059341\n",
      "14        11193347  0.019823  0.019823  0.019823  0.052053  0.052053  0.052053\n",
      "15        11215747  0.019698  0.019698  0.019698  0.070444  0.070444  0.070444\n",
      "16        11229347  0.092727  0.092727  0.092727  0.072729  0.072729  0.072729\n",
      "17        11287347  0.011793  0.011793  0.011793  0.040250  0.040250  0.040250\n",
      "18        11288547  0.010496  0.010496  0.010496  0.064731  0.064731  0.064731\n",
      "19        11324547  0.034524  0.034524  0.034524  0.025114  0.025114  0.025114\n",
      "20        11391347  0.010643  0.010643  0.010643  0.011843  0.011843  0.011843\n",
      "21        11395747  0.019790  0.019790  0.019790  0.025672  0.025672  0.025672\n",
      "22        11404347  0.019738  0.019738  0.019738  0.021415  0.021415  0.021415\n",
      "23        11405747  0.008988  0.008988  0.008988  0.017823  0.017823  0.017823\n",
      "24        11417147 -0.016696 -0.016696 -0.016696  0.020096  0.020096  0.020096\n",
      "25        11457547  0.002623  0.002623  0.002623  0.044328  0.044328  0.044328\n",
      "26        11488147  0.001813  0.001813  0.001813  0.006590  0.006590  0.006590\n",
      "27        11520747  0.040059  0.040059  0.040059  0.010482  0.010482  0.010482\n",
      "28        11524947  0.000969  0.000969  0.000969 -0.000087 -0.000087 -0.000087\n",
      "29        11544747  0.004943  0.004943  0.004943  0.004264  0.004264  0.004264\n",
      "...            ...       ...       ...       ...       ...       ...       ...\n",
      "2985187  167636430  0.015650  0.015650  0.015650  0.018373  0.018373  0.018373\n",
      "2985188  167690630  0.015650  0.015650  0.015650  0.013122  0.013122  0.013122\n",
      "2985189  167636630  0.015650  0.015650  0.015650  0.016467  0.016467  0.016467\n",
      "2985190   10834030  0.015650  0.015650  0.015650  0.037664  0.037664  0.037664\n",
      "2985191  167637430  0.015650  0.015650  0.015650 -0.001059 -0.001059 -0.001059\n",
      "2985192  167637630  0.015650  0.015650  0.015650 -0.020088 -0.020088 -0.020088\n",
      "2985193  167637230  0.015650  0.015650  0.015650  0.020676  0.020676  0.020676\n",
      "2985194   11645030  0.015650  0.015650  0.015650 -0.000486 -0.000486 -0.000486\n",
      "2985195  167689030  0.015650  0.015650  0.015650  0.019136  0.019136  0.019136\n",
      "2985196  167638630  0.015650  0.015650  0.015650  0.026235  0.026235  0.026235\n",
      "2985197  167638430  0.015650  0.015650  0.015650  0.014228  0.014228  0.014228\n",
      "2985198   14342030  0.015650  0.015650  0.015650  0.119502  0.119502  0.119502\n",
      "2985199  167638230  0.015650  0.015650  0.015650  0.045609  0.045609  0.045609\n",
      "2985200  167637830  0.015650  0.015650  0.015650  0.012929  0.012929  0.012929\n",
      "2985201  167639230  0.015650  0.015650  0.015650  0.037337  0.037337  0.037337\n",
      "2985202   14341030  0.015650  0.015650  0.015650  0.083297  0.083297  0.083297\n",
      "2985203   14341630  0.015650  0.015650  0.015650  0.079871  0.079871  0.079871\n",
      "2985204   14367630  0.015650  0.015650  0.015650  0.047948  0.047948  0.047948\n",
      "2985205  167638830  0.015650  0.015650  0.015650  0.015377  0.015377  0.015377\n",
      "2985206   12572230  0.015650  0.015650  0.015650  0.034003  0.034003  0.034003\n",
      "2985207   14460030  0.015650  0.015650  0.015650  0.036386  0.036386  0.036386\n",
      "2985208   14284830  0.015650  0.015650  0.015650  0.040841  0.040841  0.040841\n",
      "2985209   14285230  0.015650  0.015650  0.015650  0.076184  0.076184  0.076184\n",
      "2985210   14455630  0.015650  0.015650  0.015650  0.112546  0.112546  0.112546\n",
      "2985211   11117630  0.015650  0.015650  0.015650  0.031827  0.031827  0.031827\n",
      "2985212  168176230  0.015650  0.015650  0.015650  0.041135  0.041135  0.041135\n",
      "2985213   14273630  0.015650  0.015650  0.015650  0.043302  0.043302  0.043302\n",
      "2985214  168040630  0.015650  0.015650  0.015650  0.020438  0.020438  0.020438\n",
      "2985215  168040830  0.015650  0.015650  0.015650  0.044070  0.044070  0.044070\n",
      "2985216  168040430  0.015650  0.015650  0.015650  0.022369  0.022369  0.022369\n",
      "\n",
      "[2985217 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# merged on Parcelid for predicted result on test data\n",
    "\n",
    "test_dict_16 = {'Parcelid': test_parcelid16, '201610': y_pred10_16, '201611': y_pred11_16, '201612': y_pred12_16}\n",
    "test_dict_17 = {'Parcelid': test_parcelid17, '201710': y_pred10_17, '201711': y_pred11_17, '201712': y_pred12_17}\n",
    "\n",
    "df_test_16 = pd.DataFrame(data=test_dict_16)\n",
    "df_test_17 = pd.DataFrame(data=test_dict_17)\n",
    "\n",
    "df_merged = df_test_16.merge(df_test_17, left_on='Parcelid', right_on='Parcelid', how='outer')\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985217 7\n"
     ]
    }
   ],
   "source": [
    "# handle submission file\n",
    "submitfile = \"./zillow-data/sample_submission.csv\"\n",
    "\n",
    "submit_df = pd.read_csv(submitfile)\n",
    "print(submit_df.shape[0], submit_df.shape[1])\n",
    "\n",
    "assert(submit_df.shape[0] == df_merged.shape[0]), \"Error: invalid row size for submit!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submit file from df_merged\n",
    "\n",
    "df_merged.to_csv(\"./zillow-data/draft_submission3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
